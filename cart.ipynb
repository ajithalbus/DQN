{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "tmp=None\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStuff(samples):\n",
    "    k=np.array(samples)\n",
    "    g=np.array(k[:,1],dtype=np.int32)\n",
    "    dis=[[1,0],[0,1]]\n",
    "    return np.array([np.array(i) for i in k[:,0]]),np.array([dis[i] for i in g]),np.array(k[:,2]),np.array([np.array(i) for i in k[:,3]]),np.array(k[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVar(sess,name,scope):\n",
    "    if not scope:\n",
    "        return sess.run(tf.get_default_graph().get_tensor_by_name(name+':0'))\n",
    "    else:\n",
    "        return tf.get_default_graph().get_tensor_by_name(name+':0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \n",
    "    REPLAY_MEMORY_SIZE = 1000             # number of tuples in experience replay  \n",
    "    EPSILON = 0.8                         # epsilon of epsilon-greedy exploation\n",
    "    EPSILON_DECAY = 0.99                 # exponential decay multiplier for epsilon\n",
    "    HIDDEN1_SIZE = 128                     # size of hidden layer 1\n",
    "    HIDDEN2_SIZE = 128                     # size of hidden layer 2\n",
    "    EPISODES_NUM = 300                 # number of episodes to train on. Ideally shouldn't take longer than 2000\n",
    "    MAX_STEPS = 200                     # maximum number of steps in an episode \n",
    "    LEARNING_RATE = 0.0001                 # learning rate and other parameters for SGD/RMSProp/Adam\n",
    "    MINIBATCH_SIZE = 100                 # size of minibatch sampled from the experience replay\n",
    "    DISCOUNT_FACTOR = 0.9                 # MDP's gamma\n",
    "    TARGET_UPDATE_FREQ = 100             # number of steps (not episodes) after which to update the target networks \n",
    "    LOG_DIR = './logs'       \n",
    "    # Create and initialize the environment\n",
    "    def __init__(self, env):\n",
    "        self.env = gym.make(env)\n",
    "        assert len(self.env.observation_space.shape) == 1\n",
    "        self.input_size = self.env.observation_space.shape[0]        # In case of cartpole, 4 state features\n",
    "        self.output_size = self.env.action_space.n                    # In case of cartpole, 2 actions (right/left)\n",
    "    \n",
    "    # Create the Q-network\n",
    "    def initialize_network(self):\n",
    "          # placeholder for the state-space input to the q-network\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "        self.action_mask= tf.placeholder(tf.float32,[None,self.output_size])\n",
    "        self.y = tf.placeholder(tf.float32, [None,])\n",
    "        #running net\n",
    "        \n",
    "        self.Layer1 = tf.layers.dense(self.x,self.HIDDEN1_SIZE,activation=tf.nn.tanh,name='layer1')\n",
    "        self.Layer2 = tf.layers.dense(self.Layer1,self.HIDDEN2_SIZE,activation=tf.nn.tanh,name='layer2')\n",
    "        self.Q = tf.layers.dense(self.Layer2,self.output_size,name='output',activation=None)\n",
    "        \n",
    "        \n",
    "        #self.y = self.r+ self.DISCOUNT_FACTOR * tf.reduce_max(self.Q_target,axis=1)\n",
    "\n",
    "        self.y_dash = tf.reduce_sum(self.Q*self.action_mask,axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.y-self.y_dash))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\n",
    "\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.trainer = optimizer.minimize(self.loss, global_step=global_step)\n",
    "    def targetNetwork(self,t):\n",
    "        for i in range(len(self.w)):\n",
    "            if i+1==len(self.w):\n",
    "                t=np.matmul(t,self.w[i])+self.b[i]\n",
    "            else:\n",
    "                t=np.tanh(np.matmul(t,self.w[i])+self.b[i])\n",
    "            \n",
    "                \n",
    "        return t\n",
    "    def updateTarget(self):\n",
    "        #print 'TARGET COPY'\n",
    "        variableList=['layer1','layer2','output']\n",
    "        self.w=[getVar(self.session,i+'/kernel',False) for i in variableList]\n",
    "        self.b=[getVar(self.session,i+'/bias',False) for i in variableList]\n",
    "            \n",
    "        \n",
    "    def train(self, episodes_num=EPISODES_NUM):\n",
    "        global tmp\n",
    "        # Initialize summary for TensorBoard                         \n",
    "        summary_writer = tf.summary.FileWriter(self.LOG_DIR)    \n",
    "        summary = tf.Summary()    \n",
    "        # Alternatively, you could use animated real-time plots from matplotlib \n",
    "        # (https://stackoverflow.com/a/24228275/3284912)\n",
    "        \n",
    "        # Initialize the TF session\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.updateTarget()\n",
    "        self.ReplyMemory=[]\n",
    "        \n",
    "        \n",
    "        playRew=[]\n",
    "        total_steps=0\n",
    "        lost=-1\n",
    "        for episode in range(episodes_num):\n",
    "          \n",
    "            state = self.env.reset()\n",
    "\n",
    "            rew=0\n",
    "            done = False\n",
    "            episode_length=0\n",
    "            while not done:\n",
    "                \n",
    "                if np.random.rand()>self.EPSILON:\n",
    "                    action=self.env.action_space.sample()\n",
    "                    \n",
    "                else:\n",
    "                    action=self.session.run(self.Q,feed_dict={self.x:[state]})\n",
    "                    action=np.argmax(action)\n",
    "                    \n",
    "                #self.EPSILON=1-((1-self.EPSILON)*self.EPSILON_DECAY)\n",
    "                    \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                rew+=reward\n",
    "                \n",
    "                if len(self.ReplyMemory)==self.REPLAY_MEMORY_SIZE:\n",
    "                    self.ReplyMemory=self.ReplyMemory[1:] #throwing a samples\n",
    "                self.ReplyMemory.append([state,action,reward,next_state,done*1])\n",
    "                state=next_state\n",
    "\n",
    "                if len(self.ReplyMemory)<self.MINIBATCH_SIZE: #get more samples no training until then\n",
    "                    continue\n",
    "                samples=random.sample(self.ReplyMemory,self.MINIBATCH_SIZE)\n",
    "                tmp=samples\n",
    "                \n",
    "                states,action_mask,rewards,next_states,dones=getStuff(samples)\n",
    "                maxqs=np.max(self.targetNetwork(next_states),axis=1)\n",
    "                \n",
    "                y_targets=rewards+(1-dones)*self.DISCOUNT_FACTOR*maxqs\n",
    "                \n",
    "                _,lost,tp=self.session.run([self.trainer,self.loss,self.Q],feed_dict={self.x:states,\n",
    "                                                        self.action_mask:action_mask,self.y:y_targets})\n",
    "                \n",
    "                \n",
    "                if total_steps % self.TARGET_UPDATE_FREQ == 0:\n",
    "                    self.updateTarget()\n",
    "                total_steps+=1\n",
    "                episode_length+=1\n",
    "            playRew.append(rew)\n",
    "            #print self.EPSILON\n",
    "            print(\"Training: Episode = %d, Length = %d, Global step = %d loss =%.4f\" % (episode, episode_length, total_steps,lost))\n",
    "            \n",
    "            if sum(playRew[-5:])==(200*5): #early stopping\n",
    "                print 'Early stopping'\n",
    "                break\n",
    "        return playRew\n",
    "\n",
    "    # Simple function to visually 'test' a policy\n",
    "    def playPolicy(self):\n",
    "        \n",
    "        done = False\n",
    "        steps = 0\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        # we assume the CartPole task to be solved if the pole remains upright for 200 steps\n",
    "        while not done and steps < 200:     \n",
    "            self.env.render()                \n",
    "            q_vals = self.session.run(self.Q, feed_dict={self.x: [state]})\n",
    "            action = q_vals.argmax()\n",
    "            state, _, done, _ = self.env.step(action)\n",
    "            steps += 1\n",
    "        \n",
    "        return steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Training: Episode = 0, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 1, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 2, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 3, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 4, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 5, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 6, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 7, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 8, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 9, Length = 0, Global step = 0 loss =-1.0000\n",
      "Training: Episode = 10, Length = 8, Global step = 8 loss =83.6120\n",
      "Training: Episode = 11, Length = 12, Global step = 20 loss =65.2091\n",
      "Training: Episode = 12, Length = 9, Global step = 29 loss =55.4346\n",
      "Training: Episode = 13, Length = 9, Global step = 38 loss =50.0891\n",
      "Training: Episode = 14, Length = 11, Global step = 49 loss =61.5834\n",
      "Training: Episode = 15, Length = 9, Global step = 58 loss =46.8079\n",
      "Training: Episode = 16, Length = 11, Global step = 69 loss =42.3235\n",
      "Training: Episode = 17, Length = 10, Global step = 79 loss =46.6921\n",
      "Training: Episode = 18, Length = 10, Global step = 89 loss =49.2022\n",
      "Training: Episode = 19, Length = 10, Global step = 99 loss =44.3640\n",
      "Training: Episode = 20, Length = 14, Global step = 113 loss =97.5908\n",
      "Training: Episode = 21, Length = 9, Global step = 122 loss =87.3158\n",
      "Training: Episode = 22, Length = 12, Global step = 134 loss =62.1282\n",
      "Training: Episode = 23, Length = 10, Global step = 144 loss =81.2149\n",
      "Training: Episode = 24, Length = 10, Global step = 154 loss =75.7037\n",
      "Training: Episode = 25, Length = 9, Global step = 163 loss =57.5504\n",
      "Training: Episode = 26, Length = 9, Global step = 172 loss =55.9684\n",
      "Training: Episode = 27, Length = 10, Global step = 182 loss =62.8384\n",
      "Training: Episode = 28, Length = 10, Global step = 192 loss =51.4748\n",
      "Training: Episode = 29, Length = 11, Global step = 203 loss =152.9476\n",
      "Training: Episode = 30, Length = 11, Global step = 214 loss =113.9554\n",
      "Training: Episode = 31, Length = 10, Global step = 224 loss =103.9264\n",
      "Training: Episode = 32, Length = 10, Global step = 234 loss =98.7813\n",
      "Training: Episode = 33, Length = 10, Global step = 244 loss =74.2985\n",
      "Training: Episode = 34, Length = 8, Global step = 252 loss =72.7842\n",
      "Training: Episode = 35, Length = 10, Global step = 262 loss =56.7074\n",
      "Training: Episode = 36, Length = 10, Global step = 272 loss =69.9489\n",
      "Training: Episode = 37, Length = 9, Global step = 281 loss =46.3492\n",
      "Training: Episode = 38, Length = 11, Global step = 292 loss =54.0688\n",
      "Training: Episode = 39, Length = 12, Global step = 304 loss =116.6229\n",
      "Training: Episode = 40, Length = 9, Global step = 313 loss =86.2763\n",
      "Training: Episode = 41, Length = 9, Global step = 322 loss =63.7296\n",
      "Training: Episode = 42, Length = 14, Global step = 336 loss =60.4530\n",
      "Training: Episode = 43, Length = 9, Global step = 345 loss =35.9076\n",
      "Training: Episode = 44, Length = 10, Global step = 355 loss =56.8006\n",
      "Training: Episode = 45, Length = 9, Global step = 364 loss =43.1546\n",
      "Training: Episode = 46, Length = 11, Global step = 375 loss =35.5988\n",
      "Training: Episode = 47, Length = 10, Global step = 385 loss =29.4078\n",
      "Training: Episode = 48, Length = 8, Global step = 393 loss =31.0524\n",
      "Training: Episode = 49, Length = 11, Global step = 404 loss =70.0546\n",
      "Training: Episode = 50, Length = 11, Global step = 415 loss =63.1118\n",
      "Training: Episode = 51, Length = 11, Global step = 426 loss =48.7197\n",
      "Training: Episode = 52, Length = 12, Global step = 438 loss =43.3665\n",
      "Training: Episode = 53, Length = 10, Global step = 448 loss =30.0078\n",
      "Training: Episode = 54, Length = 11, Global step = 459 loss =38.0235\n",
      "Training: Episode = 55, Length = 10, Global step = 469 loss =50.9938\n",
      "Training: Episode = 56, Length = 11, Global step = 480 loss =38.3715\n",
      "Training: Episode = 57, Length = 15, Global step = 495 loss =38.9110\n",
      "Training: Episode = 58, Length = 15, Global step = 510 loss =53.0944\n",
      "Training: Episode = 59, Length = 9, Global step = 519 loss =35.3020\n",
      "Training: Episode = 60, Length = 11, Global step = 530 loss =68.6343\n",
      "Training: Episode = 61, Length = 11, Global step = 541 loss =48.6825\n",
      "Training: Episode = 62, Length = 68, Global step = 609 loss =68.3370\n",
      "Training: Episode = 63, Length = 11, Global step = 620 loss =54.8634\n",
      "Training: Episode = 64, Length = 10, Global step = 630 loss =99.7370\n",
      "Training: Episode = 65, Length = 13, Global step = 643 loss =35.2159\n",
      "Training: Episode = 66, Length = 9, Global step = 652 loss =46.5144\n",
      "Training: Episode = 67, Length = 63, Global step = 715 loss =73.3460\n",
      "Training: Episode = 68, Length = 15, Global step = 730 loss =91.8898\n",
      "Training: Episode = 69, Length = 11, Global step = 741 loss =49.9088\n",
      "Training: Episode = 70, Length = 13, Global step = 754 loss =39.3498\n",
      "Training: Episode = 71, Length = 17, Global step = 771 loss =65.0508\n",
      "Training: Episode = 72, Length = 20, Global step = 791 loss =97.8403\n",
      "Training: Episode = 73, Length = 19, Global step = 810 loss =66.8155\n",
      "Training: Episode = 74, Length = 14, Global step = 824 loss =115.8730\n",
      "Training: Episode = 75, Length = 14, Global step = 838 loss =57.4989\n",
      "Training: Episode = 76, Length = 16, Global step = 854 loss =103.9023\n",
      "Training: Episode = 77, Length = 13, Global step = 867 loss =112.3013\n",
      "Training: Episode = 78, Length = 17, Global step = 884 loss =120.8473\n",
      "Training: Episode = 79, Length = 14, Global step = 898 loss =142.0143\n",
      "Training: Episode = 80, Length = 13, Global step = 911 loss =88.0929\n",
      "Training: Episode = 81, Length = 14, Global step = 925 loss =61.3546\n",
      "Training: Episode = 82, Length = 15, Global step = 940 loss =100.1593\n",
      "Training: Episode = 83, Length = 16, Global step = 956 loss =65.4822\n",
      "Training: Episode = 84, Length = 11, Global step = 967 loss =58.9583\n",
      "Training: Episode = 85, Length = 19, Global step = 986 loss =99.9879\n",
      "Training: Episode = 86, Length = 13, Global step = 999 loss =150.0339\n",
      "Training: Episode = 87, Length = 17, Global step = 1016 loss =94.1361\n",
      "Training: Episode = 88, Length = 14, Global step = 1030 loss =90.6557\n",
      "Training: Episode = 89, Length = 13, Global step = 1043 loss =80.2071\n",
      "Training: Episode = 90, Length = 12, Global step = 1055 loss =129.0419\n",
      "Training: Episode = 91, Length = 11, Global step = 1066 loss =125.6567\n",
      "Training: Episode = 92, Length = 16, Global step = 1082 loss =76.1356\n",
      "Training: Episode = 93, Length = 13, Global step = 1095 loss =84.3450\n",
      "Training: Episode = 94, Length = 12, Global step = 1107 loss =106.5541\n",
      "Training: Episode = 95, Length = 10, Global step = 1117 loss =112.0697\n",
      "Training: Episode = 96, Length = 11, Global step = 1128 loss =79.6961\n",
      "Training: Episode = 97, Length = 18, Global step = 1146 loss =61.6038\n",
      "Training: Episode = 98, Length = 11, Global step = 1157 loss =68.7656\n",
      "Training: Episode = 99, Length = 13, Global step = 1170 loss =108.7064\n",
      "Training: Episode = 100, Length = 14, Global step = 1184 loss =53.4249\n",
      "Training: Episode = 101, Length = 16, Global step = 1200 loss =76.0485\n",
      "Training: Episode = 102, Length = 18, Global step = 1218 loss =73.2568\n",
      "Training: Episode = 103, Length = 12, Global step = 1230 loss =76.6985\n",
      "Training: Episode = 104, Length = 15, Global step = 1245 loss =91.0780\n",
      "Training: Episode = 105, Length = 16, Global step = 1261 loss =79.6398\n",
      "Training: Episode = 106, Length = 21, Global step = 1282 loss =49.3021\n",
      "Training: Episode = 107, Length = 13, Global step = 1295 loss =50.7437\n",
      "Training: Episode = 108, Length = 12, Global step = 1307 loss =86.2946\n",
      "Training: Episode = 109, Length = 12, Global step = 1319 loss =87.3643\n",
      "Training: Episode = 110, Length = 17, Global step = 1336 loss =67.3359\n",
      "Training: Episode = 111, Length = 10, Global step = 1346 loss =114.9810\n",
      "Training: Episode = 112, Length = 11, Global step = 1357 loss =80.9203\n",
      "Training: Episode = 113, Length = 15, Global step = 1372 loss =51.9413\n",
      "Training: Episode = 114, Length = 11, Global step = 1383 loss =99.4917\n",
      "Training: Episode = 115, Length = 10, Global step = 1393 loss =43.6182\n",
      "Training: Episode = 116, Length = 12, Global step = 1405 loss =164.1456\n",
      "Training: Episode = 117, Length = 23, Global step = 1428 loss =82.4327\n",
      "Training: Episode = 118, Length = 11, Global step = 1439 loss =99.0519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Episode = 119, Length = 13, Global step = 1452 loss =193.4580\n",
      "Training: Episode = 120, Length = 17, Global step = 1469 loss =306.2582\n",
      "Training: Episode = 121, Length = 17, Global step = 1486 loss =109.7306\n",
      "Training: Episode = 122, Length = 16, Global step = 1502 loss =261.3466\n",
      "Training: Episode = 123, Length = 17, Global step = 1519 loss =178.4157\n",
      "Training: Episode = 124, Length = 13, Global step = 1532 loss =88.8155\n",
      "Training: Episode = 125, Length = 17, Global step = 1549 loss =134.3235\n",
      "Training: Episode = 126, Length = 13, Global step = 1562 loss =231.5021\n",
      "Training: Episode = 127, Length = 21, Global step = 1583 loss =125.9019\n",
      "Training: Episode = 128, Length = 18, Global step = 1601 loss =98.4664\n",
      "Training: Episode = 129, Length = 36, Global step = 1637 loss =84.6991\n",
      "Training: Episode = 130, Length = 19, Global step = 1656 loss =96.2522\n",
      "Training: Episode = 131, Length = 23, Global step = 1679 loss =115.9502\n",
      "Training: Episode = 132, Length = 31, Global step = 1710 loss =245.2640\n",
      "Training: Episode = 133, Length = 17, Global step = 1727 loss =179.3938\n",
      "Training: Episode = 134, Length = 21, Global step = 1748 loss =172.5207\n",
      "Training: Episode = 135, Length = 34, Global step = 1782 loss =234.4489\n",
      "Training: Episode = 136, Length = 28, Global step = 1810 loss =235.8261\n",
      "Training: Episode = 137, Length = 16, Global step = 1826 loss =97.5857\n",
      "Training: Episode = 138, Length = 16, Global step = 1842 loss =184.2612\n",
      "Training: Episode = 139, Length = 18, Global step = 1860 loss =81.5148\n",
      "Training: Episode = 140, Length = 22, Global step = 1882 loss =33.3828\n",
      "Training: Episode = 141, Length = 31, Global step = 1913 loss =150.2655\n",
      "Training: Episode = 142, Length = 21, Global step = 1934 loss =167.5668\n",
      "Training: Episode = 143, Length = 24, Global step = 1958 loss =79.9566\n",
      "Training: Episode = 144, Length = 27, Global step = 1985 loss =69.2814\n",
      "Training: Episode = 145, Length = 31, Global step = 2016 loss =181.1180\n",
      "Training: Episode = 146, Length = 29, Global step = 2045 loss =127.8186\n",
      "Training: Episode = 147, Length = 61, Global step = 2106 loss =106.2612\n",
      "Training: Episode = 148, Length = 24, Global step = 2130 loss =93.8176\n",
      "Training: Episode = 149, Length = 49, Global step = 2179 loss =128.9404\n",
      "Training: Episode = 150, Length = 38, Global step = 2217 loss =33.8411\n",
      "Training: Episode = 151, Length = 38, Global step = 2255 loss =189.5784\n",
      "Training: Episode = 152, Length = 35, Global step = 2290 loss =136.0126\n",
      "Training: Episode = 153, Length = 40, Global step = 2330 loss =154.1248\n",
      "Training: Episode = 154, Length = 57, Global step = 2387 loss =105.7638\n",
      "Training: Episode = 155, Length = 49, Global step = 2436 loss =197.6565\n",
      "Training: Episode = 156, Length = 61, Global step = 2497 loss =117.7899\n",
      "Training: Episode = 157, Length = 49, Global step = 2546 loss =39.1784\n",
      "Training: Episode = 158, Length = 42, Global step = 2588 loss =162.8561\n",
      "Training: Episode = 159, Length = 42, Global step = 2630 loss =136.4097\n",
      "Training: Episode = 160, Length = 47, Global step = 2677 loss =121.6507\n",
      "Training: Episode = 161, Length = 93, Global step = 2770 loss =105.1196\n",
      "Training: Episode = 162, Length = 72, Global step = 2842 loss =18.2425\n",
      "Training: Episode = 163, Length = 58, Global step = 2900 loss =81.6849\n",
      "Training: Episode = 164, Length = 81, Global step = 2981 loss =119.5280\n",
      "Training: Episode = 165, Length = 101, Global step = 3082 loss =8.0331\n",
      "Training: Episode = 166, Length = 103, Global step = 3185 loss =80.6966\n",
      "Training: Episode = 167, Length = 135, Global step = 3320 loss =41.8997\n",
      "Training: Episode = 168, Length = 156, Global step = 3476 loss =6.2018\n",
      "Training: Episode = 169, Length = 195, Global step = 3671 loss =38.8121\n",
      "Training: Episode = 170, Length = 147, Global step = 3818 loss =134.1535\n",
      "Training: Episode = 171, Length = 172, Global step = 3990 loss =5.7075\n",
      "Training: Episode = 172, Length = 31, Global step = 4021 loss =48.3317\n",
      "Training: Episode = 173, Length = 198, Global step = 4219 loss =3.0737\n",
      "Training: Episode = 174, Length = 168, Global step = 4387 loss =2.6152\n",
      "Training: Episode = 175, Length = 173, Global step = 4560 loss =54.4402\n",
      "Training: Episode = 176, Length = 200, Global step = 4760 loss =51.8116\n",
      "Training: Episode = 177, Length = 200, Global step = 4960 loss =2.5482\n",
      "Training: Episode = 178, Length = 176, Global step = 5136 loss =63.8751\n",
      "Training: Episode = 179, Length = 187, Global step = 5323 loss =0.6882\n",
      "Training: Episode = 180, Length = 188, Global step = 5511 loss =1.2058\n",
      "Training: Episode = 181, Length = 165, Global step = 5676 loss =111.6206\n",
      "Training: Episode = 182, Length = 182, Global step = 5858 loss =1.0425\n",
      "Training: Episode = 183, Length = 128, Global step = 5986 loss =45.5459\n",
      "Training: Episode = 184, Length = 119, Global step = 6105 loss =45.1699\n",
      "Training: Episode = 185, Length = 137, Global step = 6242 loss =1.5205\n",
      "Training: Episode = 186, Length = 104, Global step = 6346 loss =42.6380\n",
      "Training: Episode = 187, Length = 131, Global step = 6477 loss =46.1290\n",
      "Training: Episode = 188, Length = 128, Global step = 6605 loss =39.1725\n",
      "Training: Episode = 189, Length = 120, Global step = 6725 loss =5.0039\n",
      "Training: Episode = 190, Length = 114, Global step = 6839 loss =3.0014\n",
      "Training: Episode = 191, Length = 116, Global step = 6955 loss =109.9439\n",
      "Training: Episode = 192, Length = 116, Global step = 7071 loss =2.4051\n",
      "Training: Episode = 193, Length = 122, Global step = 7193 loss =38.7402\n",
      "Training: Episode = 194, Length = 119, Global step = 7312 loss =2.6926\n",
      "Training: Episode = 195, Length = 114, Global step = 7426 loss =2.5257\n",
      "Training: Episode = 196, Length = 105, Global step = 7531 loss =2.1140\n",
      "Training: Episode = 197, Length = 107, Global step = 7638 loss =31.7956\n",
      "Training: Episode = 198, Length = 104, Global step = 7742 loss =62.9028\n",
      "Training: Episode = 199, Length = 108, Global step = 7850 loss =2.8830\n",
      "Training: Episode = 200, Length = 108, Global step = 7958 loss =59.0762\n",
      "Training: Episode = 201, Length = 40, Global step = 7998 loss =30.9392\n",
      "Training: Episode = 202, Length = 118, Global step = 8116 loss =60.0217\n",
      "Training: Episode = 203, Length = 113, Global step = 8229 loss =3.2664\n",
      "Training: Episode = 204, Length = 124, Global step = 8353 loss =3.4685\n",
      "Training: Episode = 205, Length = 124, Global step = 8477 loss =30.5226\n",
      "Training: Episode = 206, Length = 124, Global step = 8601 loss =25.9359\n",
      "Training: Episode = 207, Length = 150, Global step = 8751 loss =45.0899\n",
      "Training: Episode = 208, Length = 149, Global step = 8900 loss =27.0651\n",
      "Training: Episode = 209, Length = 154, Global step = 9054 loss =47.5108\n",
      "Training: Episode = 210, Length = 147, Global step = 9201 loss =23.3935\n",
      "Training: Episode = 211, Length = 163, Global step = 9364 loss =87.3015\n",
      "Training: Episode = 212, Length = 147, Global step = 9511 loss =1.4626\n",
      "Training: Episode = 213, Length = 157, Global step = 9668 loss =43.3438\n",
      "Training: Episode = 214, Length = 154, Global step = 9822 loss =23.2225\n",
      "Training: Episode = 215, Length = 165, Global step = 9987 loss =0.4868\n",
      "Training: Episode = 216, Length = 181, Global step = 10168 loss =23.2033\n",
      "Training: Episode = 217, Length = 192, Global step = 10360 loss =19.9910\n",
      "Training: Episode = 218, Length = 200, Global step = 10560 loss =1.4959\n",
      "Training: Episode = 219, Length = 200, Global step = 10760 loss =1.9111\n",
      "Training: Episode = 220, Length = 177, Global step = 10937 loss =18.9280\n",
      "Training: Episode = 221, Length = 190, Global step = 11127 loss =1.0353\n",
      "Training: Episode = 222, Length = 175, Global step = 11302 loss =23.0427\n",
      "Training: Episode = 223, Length = 200, Global step = 11502 loss =24.3306\n",
      "Training: Episode = 224, Length = 196, Global step = 11698 loss =12.2861\n",
      "Training: Episode = 225, Length = 200, Global step = 11898 loss =0.6596\n",
      "Training: Episode = 226, Length = 200, Global step = 12098 loss =0.8249\n",
      "Training: Episode = 227, Length = 200, Global step = 12298 loss =15.8309\n",
      "Training: Episode = 228, Length = 200, Global step = 12498 loss =148.5550\n",
      "Training: Episode = 229, Length = 200, Global step = 12698 loss =61.0629\n",
      "Early stopping\n",
      "\n",
      "Finished training...\n",
      "Check out some demonstrations\n",
      "\n",
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n",
      "('Test steps = ', 200)\n",
      "('Mean steps = ', 200)\n",
      "\n",
      "Finished.\n",
      "\n",
      "Ciao, and hasta la vista...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph()\n",
    "    # Create and initialize the model\n",
    "    dqn = DQN('CartPole-v0')\n",
    "    dqn.initialize_network()\n",
    "    \n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    tr=dqn.train()\n",
    "    print(\"\\nFinished training...\\nCheck out some demonstrations\\n\")\n",
    "    # Visualize the learned behaviour for a few episodes\n",
    "    results = []\n",
    "    for i in range(10):\n",
    "        episode_length = dqn.playPolicy()\n",
    "        print(\"Test steps = \", episode_length)\n",
    "        results.append(episode_length)\n",
    "    print(\"Mean steps = \", sum(results) / len(results))    \n",
    "\n",
    "    print(\"\\nFinished.\")\n",
    "    print(\"\\nCiao, and hasta la vista...\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
